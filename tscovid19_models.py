# -*- coding: utf-8 -*-
"""TSCOVID19_models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wI_431eLP1OfjttgPf455O2jMokLA3yI

# Comparing time series predictions of COVID-19 deaths

### The following 4 model comparison was inspired by the Coursera ["Compare time series predictions of COVID-19 deaths"](https://www.coursera.org/projects/compare-time-series-predictions-of-covid19-deaths)code along project.  As a means of improving upon these time series models that yielded modest to substandard results, a final model was employed.  That analysis encorporated an ARIMA model which derives the death predictions based on confirmed cases as well as case fatality ratios.  This approach and analysis was greatly influenced by the Ryan Tibshirani of the Delphi Research Group and the COVID tracking and foreacsting.  [See here.](https://htmlpreview.github.io/?https://github.com/cmu-delphi/covidcast-modeling/blob/master/cfr_analysis/cfr_analysis.html)
"""

#update libraries
!pip install xgboost

!pip install fbprophet
!pip install pmdarima

#Import necessary libraries

import pmdarima as pm
from statsmodels.tsa.seasonal import seasonal_decompose
from sklearn.model_selection import TimeSeriesSplit
import pandas as pd
import numpy as np
import datetime
import requests
import warnings

import matplotlib.pyplot as plt
import matplotlib
import matplotlib.dates as mdates

from xgboost import XGBRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.preprocessing import OrdinalEncoder
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.arima_model import ARIMA
from sklearn.preprocessing import StandardScaler
from fbprophet import Prophet
from fbprophet.plot import plot_plotly, add_changepoints_to_plot
from keras.models import Sequential
from keras.layers import Dense

from keras.optimizers import Adam

warnings.filterwarnings('ignore')
from google.colab import files

"""## Let's start by downloading the data and prepare the datasets"""

import pandas as pd
url_death_us = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv'

url_confirmed_us ='https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv'

confirmed_df = pd.read_csv(url_confirmed_us)
deaths_df = pd.read_csv(url_death_us)

"""First, get the data for the daily deaths in the US"""

deaths_df.head()

d = deaths_df.loc[:,'1/22/20':]  #this is also allows us to select just the columns we want
d.info()

"""Transpose the data frame"""

d = d.T
d = d.sum(axis=1)
d = d.to_list()

"""Create a new data frame with two columns, which will be our dataset:"""

dataset = pd.DataFrame(columns=['ds', 'y'])

#Convert the string dates into the datetime format, so that we can perform datetime operations on them
dates = list(deaths_df.columns[12:])
dates = list(pd.to_datetime(dates))

#assign the dates to the empty dataset
dataset['ds'] = dates
dataset['y'] = d
dataset = dataset.set_index('ds')
dataset.head()

#we'll have a look at the tail end of the deaths total as of the time we pulled this data
dataset.tail()

"""Let's plot the daily number of deaths

"""

plt.figure(figsize=(10,6))
plt.plot(dataset)
plt.savefig('Cumulative daily deaths', bbox_inches='tight', transparent=False)

"""Let's see what the daily increase looks like. To do this, we will use the `diff` method from the Dataframe object."""

plt.figure(figsize=(10,6))
plt.plot(dataset.diff())  #gives difference between present and past results
plt.savefig('Daily deaths', bbox_inches='tight', transparent=False)

#convert our dataset into the diff dataset, or the daily increase in deaths
dataset = dataset.diff()

"""We need to remove the first data point here, which will be a NaN value"""

#We're going to start with the beginning of April when the data was a bit more robust. 
#And end at the 20th of december so we have a date cutoff for future comparisons
dataset = dataset.loc['2020-05-01':'2020-12-20']

"""## Forcasting using SARIMAX (Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors) model

Before we apply the model, we need to split the dataset into training and test sets. For testing, we'll use the last two month of data.

We recognize that choosing to test which includes post-Nov holiday data will be tricky given expectations for family gatherings during this time.
"""

start_date = '2020-11-01'

train = dataset.loc[dataset.index < pd.to_datetime(start_date)]
test = dataset.loc[dataset.index >= pd.to_datetime(start_date)]

"""Now we'll focus on a SARIMAX model. First we'll tune the three hyperparameters that go into the order tuple that will minimize the error. We can use the auto_arima function in the pmdarima module to do that. This will find the optimal parameter combintation and return the best model."""

model = pm.auto_arima(train, start_p=1, start_q=1,
                      test='adf',
                      max_p=3, max_q=3,
                      m=1,
                      d=None,
                      seasonal=False,
                      start_P=0,
                      D=0,
                      trace=True,
                      error_action='ignore',
                      suppress_warnings=True,
                      stepwise=True)

print(model.summary())

"""Next, we call the fit method to optimize the model."""

#The auto_arima function yield the following hyperparemeters: 
model = SARIMAX(train, order = (2,1,2))
results = model.fit(disp=True)

"""Now let's make predictions using the model, and compare those against the values in the test set."""

sarimax_predictions = results.predict(start=start_date, end='2020-12-20', dynamic=False)

plt.figure(figsize=(10,5))
l1, = plt.plot(dataset, label = 'Observations')
l2, = plt.plot(sarimax_predictions, label = 'ARIMA')
plt.legend(handles=[l1, l2])
plt.savefig('SARIMA prediction', bbox_inches='tight', transparent=False)

"""As we anticipated, while October was very close, the overall predictions didn't track very well given the spike in deaths that occured beginning in November.

Since that we are interested in comparing between the different time series analysis approaches, we are going to use one of the validation measures: mean absolute error.
"""

print('SARIMA MAE: ', mean_absolute_error(sarimax_predictions, test['y']))

#To put this in perspective, let's look at the MAPE(mean abosulute percentage error)


error = mean_absolute_error(sarimax_predictions, test['y'])

mape = error/test.mean() 
print('Mean of the test set: {}'.format(test.mean()))
print('Our percentage error: {}'.format(mape))

"""## Forcasting using Facebook's Prophet model

Facebook prophet does not require that we specify or search for hyperparameters. The model can act as a black box that does all the required computations on its own. 

Prophet expects the data frame to have 2 columns, unlike SARIMAX.
"""

train['ds'] = train.index.values  #created an additional column for the dates as values
m= Prophet()
m.fit(train)  #prophet model has been created

"""Now let's forecast:"""

#we need to apply two methods to make a prediction
#use the .shape[0] to grab the total number of days
future = m.make_future_dataframe(periods=dataset.shape[0] - train.shape[0])  
prophet_prediction = m.predict(future)

"""Once again I'll calculate the mean absolute error for our predictions and recall that the start date of the test set will begin on November 1st."""

prophet_prediction = prophet_prediction.set_index('ds')
prophet_future = prophet_prediction.yhat.loc[prophet_prediction.index >= start_date]
print('MAE results: ', mean_absolute_error(prophet_future, test))

#Again, let's also look at the MAPE.

error = mean_absolute_error(prophet_future, test['y'])

mape = error/test.mean()
print('Mean of the test set: {}'.format(test.mean()))
print('Our percentage error: {}'.format(mape))

plt.figure(figsize=(10,5))
l1, = plt.plot(dataset, label = 'Observations')
l2, = plt.plot(prophet_future, label = 'Prophet')
plt.legend(handles=[l1, l2])
plt.savefig('Prophet prediction', bbox_inches='tight', transparent=False)

"""Again, the model was thrown off by the November spike.

## Preparing the dataset for XGBOOST and NN

Unlike the prophet and SAIMAX models, XGBOOST and neural networks(NN), are supervised machine learning models that deal with independent data points, or examples. It assumes that each data point is totally independent from the rest of the data points in the dataset.

The following is method that extracts these features from a given dataframe object.
"""

#Extract date features
def featurize(t):
  X = pd.DataFrame()
  X['day'] = t.index.day
  X['month'] = t.index.month
  X['quarter'] = t.index.quarter
  X['dayofweek'] = t.index.dayofweek
  X['weekofyear'] = t.index.weekofyear
  y = t.y
  return X, y

featurize(dataset)[0].head()

"""Create training and test datasets by splitting the dataset, and perform data normalization.

Recall we're starting with November 1st as the beginning of our testing set.
"""

X_train, y_train = featurize(
    dataset.loc[dataset.index < pd.to_datetime(start_date)])
X_test, y_test = featurize(
    dataset.loc[dataset.index >= pd.to_datetime(start_date)])

"""We perform data normalization so as to make the range of values of the features as close as possible. The range of values of dayofweek is from 1 to 7, whereas dayofyear is from 1 to 365. Having such large differences in the ranges of values will either slow down the training of the machine learning model or make it quite difficult. Chose to apply StandardScaler the normalization method."""

scaler = StandardScaler()
scaler.fit(X_train)

scaled_train = scaler.transform(X_train)
scaled_test = scaler.transform(X_test)

"""## Train the XGBOOST and NN models

First, create the XGBRegressor object which will represent the XGBOOST regression model and train using the fit method.  Finally we perform prediction using the predict method.
"""

XGBOOST_model = XGBRegressor(n_estimators=60)
XGBOOST_model.fit(scaled_train, y_train,
                  eval_set = [(scaled_train, y_train),
                  (scaled_test, y_test)],
                  verbose=True)

XGBOOST_prediction = XGBOOST_model.predict(scaled_test)

"""Let us calculate the mean absolute error for the training."""

print('XGBOOST MAE: ', mean_absolute_error(XGBOOST_prediction, y_test))
#i've changed the number of estimators from 7, to 10, to 50, to 80 and back to 60.  60 seemed to be the 
#spot where the MAE converged.

"""Creation and training of the feedforward neural network model."""

NN_model = Sequential()
NN_model.add(Dense(20, input_shape=(scaled_train.shape[1],)))  #use the columns to feed into the 20 nodes
NN_model.add(Dense(10))
NN_model.add(Dense(1))
NN_model.compile(loss='mean_absolute_error', optimizer=Adam(lr=.001))
NN_model.fit(scaled_train, y_train, validation_data=(
    scaled_test, y_test), epochs = 220, verbose = 1)
NN_prediction = NN_model.predict(scaled_test)

"""Let's compare the MAE values."""

print('XGBOOST MAE = ', mean_absolute_error(XGBOOST_prediction, y_test))
print('Prophet MAE = ', mean_absolute_error(prophet_future, test))
print('SARIMA MAE = ', mean_absolute_error(sarimax_predictions, test))
print('NN MAE = ', mean_absolute_error(NN_prediction, test))

"""Finally let us visualize the predictions of all 4 models. Note the autofmt_xdate method in matplotlib, it knows how to appropriately rotate the date labels on the x-axis."""

XGBOOST_df = pd.DataFrame({'y': XGBOOST_prediction.tolist()})
XGBOOST_df.index = y_test.index

NN_df = pd.DataFrame(NN_prediction)
NN_df.index = y_test.index
plt.figure(figsize=(30,30))
fig, axs = plt.subplots(2,2)
fig.suptitle('Compare SARIMA, Prophet, XGBOOST, and Neural Networks(NN)', fontsize=12)

axs[0,0].plot(dataset.tail(50))
axs[0,0].plot(sarimax_predictions.tail(50))
axs[0,0].set_title('SARIMA')
axs[0,1].plot(dataset.tail(50))
axs[0,1].plot(prophet_future.tail(50))
axs[0,1].set_title('Prophet')
axs[1,0].plot(dataset.tail(50))
axs[1,0].plot(XGBOOST_df.tail(50))
axs[1,0].set_title('XGBOOST')
axs[1,1].plot(dataset.tail(50))
axs[1,1].plot(NN_df.tail(50))
axs[1,1].set_title('Neural Network')

for ax in fig.get_axes():
  ax.label_outer()
fig.autofmt_xdate()

plt.savefig('Comparison.png',
            bbox_inches='tight', transparent=False)

files.download('Comparison.png');

"""The XGBOOST model fared the best albiet still suffered from the same spikes we witnessed in November and December.  If instead of directly predicting the path of the death time series, we looked at this problem as dependent on the number of confirmed cases and the case fatality ratio.  Inspired from the [Delphi Research Group](https://delphi.cmu.edu/about/) (describe themselves as "based out of Carnegie Mellon University dedicated to developing the theory and practice of epidemic tracking and forecasting", I take the alternative approach to the SARIMA, Prophet, XGBOOST, and NN and model the confirmed cases as a predictor for future deaths and apply ARIMA models."""

confirmed_df = pd.read_csv(url_confirmed_us)
deaths_df = pd.read_csv(url_death_us)

#Preprocess the data
dd = deaths_df.loc[:,'1/22/20':]
dc = confirmed_df.loc[:,'1/22/20':] 

#confirmed data
dc = dc.T
dc = dc.sum(axis=1)
dc = dc.to_list()
dc_df = pd.DataFrame(columns=['ds', 'confirmed'])
dates = list(confirmed_df.columns[11:])
dates = list(pd.to_datetime(dates))
dc_df['ds'] = dates
dc_df['confirmed'] = dc
dc_df = dc_df.set_index('ds')

#deaths data
dd = dd.T
dd = dd.sum(axis=1)
dd = dd.to_list()
dd_df = pd.DataFrame(columns=['ds', 'deaths'])
dates = list(deaths_df.columns[12:])
dates = list(pd.to_datetime(dates))
dd_df['ds'] = dates
dd_df['deaths'] = dd
dd_df = dd_df.set_index('ds')

#combined data
df = pd.merge(dc_df, dd_df, on='ds', how='outer')

dd_df.tail()

#Convert to the daily changes and smooth out data with a 7day trailing ma
df = df.diff()
df = df.rolling(window=7).mean() #7day trailing average window

"""These time series models naturally didn't track well as the curve in both the confirmed cases and deaths spike in November and developed an exponential rise (which devaited substantially from the summmer months)

An alternative method would be to track the confirmed cases and instead calculate an adjusted case fatality ratio(CFR) on a lagged death impact.  In the next few cells I'll investigate the appropriate CFR and apply highest correlated lag in days.
"""

#calculate the case fatality rate (cfr) over the time period
cfr = round((df['deaths'] / df['confirmed']),4)*100
df['cfr'] = cfr

#Looking at the most recent case fatality rates and end analysis at 12/23/20
df = df.loc[:'12/20/20']
df.tail()

#choose start date
cfr_df = df.loc['2020-05-01':'2020-12-20']

title = 'US Case Fatality Ratio'
ylabel='Case Fatality Ratio (%)'
xlabel='Date'
mylabels = ['Case Fatality Ratio']

avg_cfr = cfr_df['cfr'].loc['2020-07-01':].mean()/100
print('Average CFR since July has been: {:%}'.format(avg_cfr))
ax = cfr_df['cfr'].plot(legend=True,figsize=(10,6))
ax.grid()
ax.autoscale(axis='x',tight=True)
ax.set_title(label=title,loc='left')
ax.set_title(label='May 1 - Dec 20', loc='right')
ax.set(xlabel=xlabel, ylabel=ylabel)
ax.legend(labels=mylabels)

plt.axhline(y=(avg_cfr * 100), color='k', linestyle='--')

plt.savefig('case_fatality_ratio.png',dpi=60, bbox_inches = "tight")
files.download('case_fatality_ratio.png');

from scipy.stats import pearsonr
#first try out data from May, then we can review how the curves change over time
lag_test = df.loc['2020-05-01':'2020-12-20'].copy()
shift_df = pd.DataFrame(columns = ['Lag','Correlation'])

for shift in range(0,40,1):
    case_lag = df['confirmed'].shift(shift)
    shifts = str(shift) 
    lag_test['Lag'+shifts] = case_lag
    corr, _ = pearsonr(lag_test['deaths'], lag_test['Lag'+shifts])
    shift_df= shift_df.append({'Lag':shift, 'Correlation': corr}, ignore_index=True)

#plot our curves
shift_df.set_index('Lag',inplace=True)
title = 'US Correlation Between Lagged Cases and Deaths'
ylabel='Correlation (%)'
xlabel='Lag (Days)'
mylabels = ['Pearson Correlation']

ax = shift_df.plot(legend=True,figsize=(8,6), linestyle='-.')
ax.grid()
ax.autoscale(axis='x',tight=True)
ax.set_title(label=title,loc='left')
ax.set_title(label='May 1 - Dec 20', loc='right')
ax.set(xlabel=xlabel, ylabel=ylabel)
ax.legend(labels=mylabels)
#reduce the name of tick marks on each axis
plt.locator_params(axis="x", nbins=8)
plt.locator_params(axis="y", nbins=4)

y = shift_df['Correlation']
x = shift_df.index
#draw a line at the optimal lag number
max_x = x[y.argmax()]
plt.axvline(x=(max_x), color='r', linestyle='-')

plt.savefig('correlation_lags.png',dpi=60, bbox_inches = "tight")
files.download('correlation_lags.png');

df = df.loc['2020-04-01':]  #give an extra month to account for the lags
df.tail()

#Select lags 20,24, and 28 days
df['lag20'] = df['confirmed'].shift(20)
df['lag24'] = df['confirmed'].shift(24)
df['lag28'] = df['confirmed'].shift(28)

#Calc CFR at each lag
c20 = df['deaths'] / df['lag20']
c24 = df['deaths'] / df['lag24']
c28 = df['deaths'] / df['lag28']
df['L20'] = c20 * 100
df['L24'] = c24 * 100
df['L28'] = c28 * 100

title = 'US Case Fatality Ratio'
ylabel='Case Fatality Ratio (%)'
xlabel='Date'
mylabels = ['Lag20','Lag24','Lag28']

corrlag_df = df[['L20','L24','L28']]
#As before, we'll see the CFR to '1.5% and see if that's still holding or if that trend has changed.
corrlag_df=corrlag_df.loc['2020-09-01':'2020-12-20']
ax = corrlag_df.plot(legend=True,figsize=(12,6))
ax.grid()
ax.autoscale(axis='x',tight=True)
ax.set_title(label=title,loc='left')
ax.set_title(label='Sep 1 - Dec 20', loc='right')
ax.set(xlabel=xlabel, ylabel=ylabel)
ax.legend(labels=mylabels)
last_val = round(corrlag_df.tail(1).mean(axis=1)[0],2)
print("Latest average CFR: {}".format(last_val))

plt.axhline(y=1.5, color='b', linestyle='--');

plt.savefig('cfr_lag_trends.png',dpi=60, bbox_inches = "tight")
files.download('cfr_lag_trends.png');

#CFR is now the column headers and the lags will be the indexes with the data representing the mse values
#so an mse gets calcd 
from sklearn.metrics import mean_absolute_error     

lag_test = df.loc['2020-07-01':].copy()
mae_df = pd.DataFrame(columns = ['Ratio','Lag','MAE'])
cfr_vals = [1.4,1.6,1.8,2.0]
for cfr in cfr_vals:
    for lag in range(10,30,1):
        cfrs = str(cfr)
        cases = df['confirmed'].shift(lag)
        lag_test['cases'] = cases
        pred = cfr * lag_test['cases'] / 100
        true = lag_test['deaths']
        mae = mean_absolute_error(true, pred)
        mae_df = mae_df.append({'Ratio': cfrs, 'Lag': lag,'MAE': mae}, ignore_index=True)


mae_pivot = mae_df.pivot(index='Lag', columns = 'Ratio', values='MAE')

title = 'US FORECAST ERRORS'
ylabel='Mean Absolute Error (Deaths)'
xlabel='Lags'

ax = mae_pivot.plot(legend=True,figsize=(8,5), linestyle='--')
ax.grid()
ax.xaxis.get_major_locator().set_params(integer=True)  #clean up x-axis
ax.autoscale(axis='x',tight=True)
ax.set_title(label=title,loc='left')
ax.set_title(label='Jul 1 - Dec 20', loc='right')
ax.set(xlabel=xlabel, ylabel=ylabel)
ax.legend(title='CFR',loc='best', bbox_to_anchor=(1, 0.5))

plt.savefig('cfr_forecast_errors.png',dpi=60, bbox_inches = "tight")
files.download('cfr_forecast_errors.png');

cases = df.loc['2020-05-01':].copy()
case_lags = df['lag20'].loc['2020-05-01':].copy()
deaths_adj = 1.6 * case_lags / 100

deaths_adj.head()

"""We can clearly see that the MAE is significantly lower than the prior models.

The 1.6 CFR curve appears to have the lowest MAE at lag 20.  We will work with those parameters in our predictions.  First let's have a look at how the Delphi Group viewed the dataset through their time frame up to Nov 15th (have their metrics held true?)
"""

#But what about the data presented by the Delphi Group which capped results in Nov
from sklearn.metrics import mean_absolute_error     

lag_test = df.loc['2020-07-01':'2020-11-15'].copy()
mae_df = pd.DataFrame(columns = ['Ratio','Lag','MAE'])
cfr_vals = [1.3,1.5,1.7,2.0]
for cfr in cfr_vals:
    for lag in range(10,30,1):
        cfrs = str(cfr)
        cases = df['confirmed'].shift(lag)
        lag_test['cases'] = cases
        pred = cfr * lag_test['cases'] / 100
        true = lag_test['deaths']
        mae = mean_absolute_error(true, pred)
        mae_df = mae_df.append({'Ratio': cfrs, 'Lag': lag,'MAE': mae}, ignore_index=True)

mae_pivot = mae_df.pivot(index='Lag', columns = 'Ratio', values='MAE')

title = 'US Forecast Errors \n from July 1st to November 15th (Delphi research period)'
ylabel='Mean Absolute Error (Deaths)'
xlabel='Lag'

ax = mae_pivot.plot(legend=True,figsize=(8,5),title=title, linestyle='--')
ax.grid()
ax.xaxis.get_major_locator().set_params(integer=True)  #clean up x-axis
ax.autoscale(axis='x',tight=True)
ax.set(xlabel=xlabel, ylabel=ylabel)
ax.legend(title='CFR',loc='best', bbox_to_anchor=(1, 0.5));

"""As we can see above, their lags were shorter even prior to the Nov-Dec spike however the CFR rates consistently showed a lower MAE at the 1.6 rate

### Let's make a prediction ###

*For our analysis, since we've seen case fatality ratios trending up, we'll apply the latest CFR of 1.6% (which is higher than the average of 1.45% since the July) to forecast 16 days(shortest lag found by Delphi), 20 days(my lowest error) and 24 days into the future which was the longest lag to display a significant correlation.
"""

model = pm.auto_arima(df['confirmed'], start_p=1, start_q=1,
                      test='adf',
                      max_p=3, max_q=3,
                      m=1,
                      d=None,
                      seasonal=False,
                      start_P=0,
                      D=0,
                      trace=True,
                      error_action='ignore',
                      suppress_warnings=True,
                      stepwise=True)

print(model.summary())

#Create our forecasts

#Set the CFR to 1.6%
cfr = 1.6

# ARIMA
from statsmodels.tsa.arima_model import ARIMA,ARIMAResults

predicts_16 = pd.DataFrame()  #delphi lag
predicts_20 = pd.DataFrame()  #my analysis lag
predicts_24 = pd.DataFrame()  #upperbound and highest correlation since May

data = df['confirmed']
# fit model

model = ARIMA(data,order=(2,1,0))
results = model.fit()

fcast16 = results.predict(1,len(df)+16,typ='levels').rename('Confirmed predictions') #looking out 16days
fcast20 = results.predict(1,len(df)+20,typ='levels').rename('Confirmed predictions') #looking out 20days
fcast24 = results.predict(1,len(df)+24,typ='levels').rename('Confirmed predictions') #looking out 26days

predicts_16['deaths16'] = fcast16.shift(16) * cfr / 100
predicts_20['deaths20'] = fcast20.shift(20) * cfr / 100
predicts_24['deaths24'] = fcast26.shift(24) * cfr / 100

predicts_16.index.names = ['ds']
predicts_20.index.names = ['ds']
predicts_24.index.names = ['ds']

"""Let's analyze the error to compare to the prior models - For the adjusted lagged model, we'll apply the 20 day lag which showed the lowest MAE."""

confirmed_prediction = results.predict(1, len(df), typ='levels')
death_predictions = confirmed_prediction.shift(20) * cfr / 100
death_predictions.index.names = ['ds']
fcst_test  = death_predictions.iloc[-len(test):]

print('ARIMA 20-day lagged MAE: ', mean_absolute_error(fcst_test, test))
print('XGBOOST MAE = ', mean_absolute_error(XGBOOST_prediction, y_test))
print('Prophet MAE = ', mean_absolute_error(prophet_future, test))
print('SARIMA MAE = ', mean_absolute_error(sarimax_predictions, test))
print('NN MAE = ', mean_absolute_error(NN_prediction, test))

import matplotlib.ticker as ticker
formatter = ticker.StrMethodFormatter('{x:,.0f}')

fcst = predicts_16.loc['2020-07-01':].copy()

# Plot predictions against known values
title = 'US 16-day ahead forecast - CFR @ 1.6%'
ylabel='Number of deaths'
xlabel='Date'
mylabels = ['Actual', 'Prediction']

df_plot = df.loc['2020-07-01':].copy()
ax = df_plot['deaths'].plot(legend=True,figsize=(12,6))
fcst['deaths16'].plot(legend=True)
ax.autoscale(axis='x',tight=True)
ax.set_title(label=title,loc='left')
ax.set(xlabel=xlabel, ylabel=ylabel)
ax.legend(labels=mylabels)
ax.yaxis.set_major_formatter(formatter)

plt.savefig('fcst16day_plot.png',dpi=60, bbox_inches = "tight")
files.download('fcst16day_plot.png');

import matplotlib.ticker as ticker
formatter = ticker.StrMethodFormatter('{x:,.0f}')

fcst = predicts_20.loc['2020-07-01':].copy()

# Plot predictions against known values
title = 'US 20-day ahead forecast - CFR @ 1.6%'
ylabel='Number of deaths'
xlabel='Date'
mylabels = ['Actual', 'Prediction']

df_plot = df.loc['2020-07-01':].copy()
ax = df_plot['deaths'].plot(legend=True,figsize=(12,6))
fcst['deaths20'].plot(legend=True)
ax.autoscale(axis='x',tight=True)
ax.set_title(label=title,loc='left')
ax.set(xlabel=xlabel, ylabel=ylabel)
ax.legend(labels=mylabels)
ax.yaxis.set_major_formatter(formatter)

plt.savefig('fcst20day_plot.png',dpi=60, bbox_inches = "tight")
files.download('fcst20day_plot.png');

import matplotlib.ticker as ticker
formatter = ticker.StrMethodFormatter('{x:,.0f}')

fcst = predicts_24.loc['2020-07-01':].copy()

# Plot predictions against known values
title = 'US 24-day ahead forecast - CFR @ 1.6%'
ylabel='Number of deaths'
xlabel='Date'
mylabels = ['Actual', 'Prediction']

df_plot = df.loc['2020-07-01':].copy()
ax = df_plot['deaths'].plot(legend=True,figsize=(12,6))
fcst['deaths24'].plot(legend=True)
ax.autoscale(axis='x',tight=True)
ax.set_title(label=title,loc='left')
ax.set(xlabel=xlabel, ylabel=ylabel)
ax.legend(labels=mylabels)
ax.yaxis.set_major_formatter(formatter)

plt.savefig('fcst24day_plot.png',dpi=60, bbox_inches = "tight")
files.download('fcst24day_plot.png');

#Calculate the totals deaths in each foreacst
sum16 = predicts_16.loc['2020-12-21':].sum()
death_forecast16 = (dd_df.deaths.max() + sum16).astype(int)[0]
sum20 = predicts_20.loc['2020-12-21':].sum()
death_forecast20 = (dd_df.deaths.max() + sum20).astype(int)[0]
sum24 = predicts_24.loc['2020-12-21':].sum()
death_forecast24 = (dd_df.deaths.max() + sum24).astype(int)[0]

dd_df = dd_df[:'2020-12-20']
current_dt = dd_df.index[-1].strftime('%Y-%m-%d')
date16 = predicts_16.index[-1].strftime('%Y-%m-%d')
date20 = predicts_20.index[-1].strftime('%Y-%m-%d')
date24 = predicts_24.index[-1].strftime('%Y-%m-%d')

predictions = [ (current_dt, dd_df.deaths.max()),
               (date16, death_forecast16),
               (date20, death_forecast20),
               (date24, death_forecast24)  ]

death_forecast = pd.DataFrame(predictions, columns = ['Date', 'Total Deaths'] )
death_forecast.set_index('Date', inplace=True)

"""## Plot out the forecast and annotate each prediction"""

#forecast plot
import datetime as dt
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import matplotlib.ticker as ticker
import pandas.plotting as plotting
formatter = ticker.StrMethodFormatter('{x:,.0f}')

title = 'US forecasted total deaths \n Lags of 16, 20, and 24 days'
ylabel='Total Deaths'
xlabel='Forecasted Date'

death_forecast.index = pd.to_datetime(death_forecast.index) #convert back to datetime
xs = death_forecast.index
ys = death_forecast['Total Deaths'].tolist()

fig, ax = plt.subplots(figsize=(8,6))

#add formatted dataframe table to the plot
table = death_forecast.copy()
table['Total Deaths'] = table['Total Deaths'].apply(lambda x: "{:,}".format(x))
table.index = table.index.strftime('%Y-%m-%d')
table.sort_values(by='Total Deaths', ascending=False, inplace=True)
plotting.table(ax,table,
               loc='lower right', colWidths=[0.2, 0.2, 0.2],
               bbox = [1.2, 0.7, 0.2, 0.3]) #trial and error placement outside figure

#annotate each forecast point
ax.plot_date(xs, ys, linestyle='--')
text_pts = [(25,5),(-50,0),(0,-15),(-60,-5)]
for pt,(x,y) in enumerate(zip(xs,ys)):
  label =  "{:,}".format(y)
  ax.annotate(label, (mdates.date2num(xs[pt]), ys[pt]), 
              xytext=(text_pts[pt]),
              textcoords='offset points')
ax.grid()
#call out the last reported figure as of analysis
ax.annotate('Actual reported',
            (mdates.date2num(xs[0]), ys[0]), 
            xytext=(-15,50),
            textcoords='offset points',
            arrowprops=dict(arrowstyle='-|>'))

fig.autofmt_xdate()
ax.yaxis.set_major_formatter(formatter)

ax.set_title(label=title,loc='left')
ax.set_title(label='Dec 20 - Jan 14', loc='right')
ax.set(xlabel=xlabel, ylabel=ylabel)

plt.savefig('death_forecast_plot.png',dpi=60, bbox_inches = "tight")
files.download('death_forecast_plot.png');

